\section{Continuous Time Markov Chain}

\begin{exercise}[from Exercise 20.3 of Markov Chains and Mixing Times]
  Let $T_1, T_2, \cdots$ be an i.i.d sequence of exponential random variables of rate $\mu$,
  let $S_k = \sum_{i=1}^k T_i$, and let $N_t = \max\{k: S_k\leq t\}$. \\
  (a) Show that $S_k$ has a gamma distribution with shape parameter $k$ and rate parameter $\mu$,
  i.e. its density function is
  \begin{align*}
    f_k(s) = \frac{\mu^ks^{k-1}e^{-\mu s}}{(k-1)!}
  \end{align*}
  (b) Show by computing $\Pr\{S_k\leq t < S_{k+1}\}$ that $N_t$ is a Possion random variable
\end{exercise}
\begin{proof}
  (a): First we know that $f_{T_1}(x) = f_{T_2}(x) = \mu e^{-\mu x}$ when $x \geq 0$.
  Then
  \begin{align*}
  f_{T_1 + T_2}(s) &= \int_{-\infty}^{\infty} f_{T_1}(t) f_{T_2}(s - t) \mathrm{d}t \\
                   &= \int_{0}^{s} \mu e^{-\mu t} \mu e^{-\mu (s - t)} \mathrm{d}t\\
                   &= \int_{0}^{s} \mu^2 e^{-\mu s}\mathrm{d}t\\
                   &= \left.\mu^2e^{-\mu s} t \right|^s_0  \\
                   &= \mu^2e^{-\mu s} s
  \end{align*}
  And its very easy to generalize this result to the sum of $k$ random variables. \\
  (b): Since $S_k$ and $T_{k+1}$ are two independent random variables, we have:
  \begin{align*}
    \Pr\{S_k \leq t < S_{k+1} &= \Pr\{S_k \leq t < S_k + T_{k+1}\} \\
                              &= \int_0^t f_k(s)\mathrm{d}s \cdot \Pr\{T_{k+1} > t - s\} \\
                              &= \int_0^t \frac{\mu^k s^{k-1} e^{-\mu s}}{(k-1)!}\mathrm{d}s \cdot e^{-\mu (t - s)} \\
                              &= \int_0^t \frac{\mu^ks^{k-1}e^{-\mu t}}{(k-1)!}\mathrm{d}s \\
                              &= \left.\frac{\mu^ks^ke^{-\mu t}}{k!} \right|^t_0 \\
                              &= \frac{(\mu t)^ke^{-\mu t}}{k!} \qedhere
  \end{align*}
\end{proof}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../../notebook"
%%% End:
