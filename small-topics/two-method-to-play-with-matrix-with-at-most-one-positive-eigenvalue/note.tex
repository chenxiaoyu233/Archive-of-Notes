\documentclass{article}
%\usepackage{geometry}
%\geometry{a4paper, scale=0.8}

\usepackage{amsmath, amssymb, amsthm}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{define}{Define}[section]
\newtheorem{fact}{Fact}[section]

\usepackage{hyperref}

\title{Playing with matrix which has at most 1 positive eigenvalue}
\author{Xiaoyu Chen}
\date{}

% simplify writting
\def\diag#1{(\mathrm{diag}\,#1)}
\def\<{\langle}
\def\>{\rangle}

\begin{document}
\maketitle
\section{Background}
Recently, I am focusing on sampling matroid basis.
As I notices, there are two group of people considering this problem this year \cite{anari2019log}\cite{cryan2019modified}.
The interesting things is that both of them use some techniques to analyze the eigenvalue of some specific matrix (i.e. the matrics with at most 1 positive eigenvalue).

The main idea is transform the matrix we want to analyze to another matrix which has some specific properties and is easy to be analyze.
Then, find the relationshiop between the matrix we want to analyze and the special matrix.

In this note, I just want to summary and compare the methods they use.


\section{Preliminary}
Say, we have a real symmetry matrix $M \in \mathbb{R}^{n\times n}$, with non-negetive entries, and has most $1$ positive eigenvalue.
Moreover, each line of $M$ sum up to be $> 0$.
The matrix which has the special properties is denoted by $P$.
$\vec{w}$ is a special vector which we are going to use to construct $P$.
Particularly, $\vec{w}(i) = \sum_{j=1}^n M_{ij}$.

\section{Method used in \cite{anari2019log}}
In \cite{anari2019log}, they write $M = \diag{\vec{w}}P$, which could be seen as a normalize factor for each line of $M$.
Then, they define a new inner product to play with $P$'s eigenvalue.
Note that here, $P$ could be seen as a transition matrix of some markov chain.
\begin{define}
  \[\<\phi, \varphi\>_P := \varphi^T \diag{\vec{w}} \phi\]

  $\triangle$: Note that the normal inner product is $\<\phi, \varphi\> := \varphi^T\phi$.
\end{define}

\begin{fact}[transform between inner product spaces]
  \label{fact:trans}
  \[\<\phi, M\varphi\> = \<\phi, P\varphi\>_P\]

  $\triangle$: This fact could be seen as a transition between different inner product spaces.
\end{fact}

\begin{fact}
  $P$ is self-adjoint w.r.t. $\<\cdot, \cdot\>_P$.
\end{fact}
\begin{proof}
  \begin{align*}
    \<\phi, P\varphi\>_P
    &= (P\varphi)^T\diag{\vec{w}}\phi \\
    &= \varphi^TP^T\diag{\vec{w}}\phi \\
    &= \varphi^T(\diag{\vec{w}}P)^T\phi \\
    &= \varphi^TM\phi\\
    &= \<M\phi, \varphi\>
  \end{align*}
  \begin{align*}
    \<P\phi, \varphi\>_P
    &= \varphi^T \diag{\vec{w}}P\phi \\
    &= \varphi^T M\phi \\
    &= \<M\phi, \varphi^T\>
  \end{align*}
\end{proof}

\begin{fact}
  $P$ has at most 1 eigenvalue.
\end{fact}
\begin{proof}
Since $P = \diag{\vec{w}}^{-1} M$ and $\diag{\vec{w}}^{-1}$ is PSD, by using Lemma 2.5 in \cite{anari2019log}, we know that $P$ has at most 1 positive eigenvalue just like $M$.
\end{proof}

\begin{fact}
  $P$ has exactly 1 eigenvalue.
  
  (i.e. eigenvalue 1 with eigenvector $\vec{1}$).
\end{fact}

\begin{fact}
  $P$'s eigenvectors forms a orthonormal basis of $\mathbb{R}^n$ w.r.t. $\<\cdot, \cdot\>_P$.
\end{fact}
\begin{proof}
Since $P$ is self-adjoint w.r.t. $\<\cdot, \cdot\>_P$, we know that $P$'s eigenvectors could form an orthonormal basis of $\mathbb{R}^n$ w.r.t. $\<\cdot, \cdot\>_P$ (this property is analyzed explicitly in my another note).
\end{proof}

\subsection{Application}
After knowing all these facts, we could have fun palying with $P$'s eigenvalues and orthonormal eigenvectors w.r.t. $\<\cdot, \cdot\>_P$.

For example, the useful [Courant-Fischer Theorem] could be applied w.r.t. $\<\cdot, \cdot\>_P$ since we have orthonormal of $P$ in this inner product space. And it sould be convenient to give the Loewner order between $P$ and some other matrix in this inner product space, and then transform back using Fact \ref{fact:trans}.

Since the choice of inner product would not change the eigenvalue of the matrix.
And it maybe easier to analyze the Loewner order between two matrics.
Sometimes, we just find the Loewner order w.r.t. some specific inner product, and then we have the relationship between the eigenvalues of two matrics. (See the Proof of Theorem 3.3 in \cite{anari2019log} for more detail).

\section{Method used in \cite{cryan2019modified}}
The authors in \cite{cryan2019modified} use a more explicit but less elegant method to play with this kind of matrics.

\begin{fact}
  Note that $M$ is a real symmetry matrix, so we could write $P$ as
  \[P = D^{-1/2}MD^{-1/2}\]
  where $D = \diag{\vec{w}}$.
\end{fact}
  In this construction, $P$ could no longer be seen as the transition matrix of some markov chain. But we have the following facts to play with $P$.
\begin{fact}
  $P$ has at most 1 eigenvalue.
\end{fact}
\begin{proof}
  refer to Lemma 2.3 in \cite{anari2019log}.
\end{proof}
\begin{fact}
  $P$ has eigenvalue $1$ with eigenvector $\vec{\sqrt{w}}$, where $\vec{\sqrt{w}}(i) = \sqrt{\vec{w}(i)}$.
\end{fact}
\begin{proof}
  \begin{align*}
    P\vec{\sqrt{w}}
    &= D^{-1/2}MD^{-1/2}\vec{\sqrt{w}} \\
    &= D^{-1/2}M\\
    \left[
    \begin{array}{c}
      \vdots \\
      \vec{\sqrt{v}}(i) \\
      \vdots
    \end{array}
    \right]
    &= \left[
      \begin{array}{ccc}
        \ddots&& \\
        &\sqrt{\vec{w}}(i)&\\
        &&\ddots
      \end{array}
      \right]
    \left[
           \begin{array}{ccc}
             \ddots &   &  \\
             M(i, 1) & \cdots & M(i, n) \\
              &  &  \ddots\\
           \end{array}
    \right] \\
    &= \vec{\sqrt{w}}
  \end{align*}
\end{proof}

\begin{fact}
  $P$ is also a real symmetry matrix like $M$.
  So, $P$ has $n$ orthonormal eigenvectors which could be used as a basis of $\mathbb{R}^n$.
\end{fact}

\begin{fact}
  If $P$ is a real symmetry matrix, and $g_i, i\in [n]$ is the orthonormal eigenvectors of $P$, then we have:
  \[P = \sum_{i=1}^n \lambda_i g_i g_i^T\]
\end{fact}
\begin{proof}
  First, note that $P$ could be diagonalize, since $P$ is a real symmetry matrix.
  Particularly, if $G = [g_1, g_2, \cdots g_n]$, then
  \[\Lambda = G^TPG\]
  So, at the same time, we have:
  \begin{align*}
    P
    &= G\Lambda G^T \\
    &= \sum_i G\Lambda_i G^T, \hspace{0.5cm} \Lambda_i = \left[
      \begin{array}{ccc}
        \ddots & & \\
               &\lambda_i& \\
               & & \ddots
      \end{array}
    \right] \\
    &= \sum_i
      \left[
      \begin{array}{ccc}
        \cdots & g_i(1)& \cdots \\
        \vdots & \vdots& \vdots \\
        \cdots & g_i(n)& \cdots
      \end{array}
       \right]
       \left[
      \begin{array}{ccc}
        \ddots & &  \\
         & \lambda_i&  \\
         & & \ddots
      \end{array}
       \right]
      \left[
      \begin{array}{ccc}
        \ddots & &  \\
        g_i(1) & \cdots& g_i(n) \\
         & & \ddots
      \end{array}
             \right] \\
    &= \sum_i \lambda_i g_i g_i^T \qedhere
  \end{align*}
\end{proof}

\subsection{Application}
Just like above:
\begin{enumerate}
\item Courant-Fischer Theorem
\item Loewner order
\end{enumerate}

\section{Summary}
Both method here do not analyze $M$ directly.
Instead, they find a matrix $P$ which has exactly 1 positive eigenvalue. (i.e. eigenvalue 1 with corresponding eigenvector).
At the same time, they maintain the eigenvectors of $P$ still to be orthonormal just lik $M$.

\bibliographystyle{unsrt}
\bibliography{note}
\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
