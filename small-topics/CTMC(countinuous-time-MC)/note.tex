\documentclass{article}
\usepackage[usenames]{color}
\usepackage{amsmath, amssymb, amsthm}
\usepackage[utf8]{inputenc}
\usepackage{algorithm2e}
\usepackage{xcolor}
\usepackage{tikz}
\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=cyan,
}

\newtheorem{fact}{Fact}[section]
\newtheorem{define}{Definition}[section]
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}{Lemma}[section]
\newtheorem{remark}{Remark}[section]

\title{Notes for Continuous Time Markov Chains}
\author{Xiaoyu Chen}
\date{}

\begin{document}
\maketitle

\section{Basic Idea}
For a Markov chain $P$, we may think of it as a function $f^P: \mathbb{Z}_{\geq 0}\to \mathbb{R}^{n\times n}$ defined as
\[f^P(t) = P^t\].
Now, to make $P$ a continuous Markov chain, what we want to achieve is to find a new function $\tilde{f^P}: \mathbb{R}_{\geq 0} \to \mathbb{R}^{n\times n}$ while make sure that $\tilde{f^P}(t) = f^P(t)$ when $t\in \mathbb{Z}_{\geq 0}$.

Note that if
\begin{align*}
  e^Q &= I + Q + \frac{Q^2}{2!} + \frac{Q^3}{3!} + \cdots \\
      &= \sum_{n = 0}^\infty \frac{Q^n}{n!}
\end{align*},
and if $e^Q = P$, then we could define the function $\tilde{f^P}$ as
\[\tilde{f^P}(t) = e^{tQ}\]

\begin{fact}
  $e^{Q_1 + Q_2} = e^{Q1}e^{Q2}$
\end{fact}
\begin{proof}
  \begin{align*}
    e^{Q_1 + Q_2}
    &= \sum_{n=0}^\infty \frac{(Q_1 + Q_2)^n}{n!} \\
    &= \sum_{n=0}^\infty \frac{1}{n!} \sum_{k=0}^n \binom{n}{k} Q_1^k Q_2^{n-k} \\
    &= \sum_{n=0}^\infty \frac{1}{n!} \sum_{k=0}^n \frac{n!}{(n-k)!k!} Q_1^k Q_2^{n-k} \\
    &= \sum_{k=0}^\infty \frac{Q_1^k}{k!} \sum_{n=k}^\infty \frac{Q_2^{n-k}}{(n-k)!} \\
    &= e^{Q_1} e^{Q_2}
  \end{align*}
\end{proof}

\begin{fact}
  $e^I = e \cdot I$ and thus $e^{t(Q + I)} = e^t e^{tQ}$.
\end{fact}

\begin{fact}
  $\frac{\mathrm{d}}{\mathrm{d} t} e^{tQ} = Q e^{tQ} = e^{tQ} Q$
\end{fact}
\begin{proof}
  \begin{align*}
    \frac{\mathrm{d}}{\mathrm{d} t} e^{tQ}
    &= \frac{\mathrm{d}}{\mathrm{d} t} \sum_{n=0}^\infty \frac{t^nQ^n}{n!} \\
    &= \sum_{n=1}^\infty \frac{\mathrm{d}}{\mathrm{d} t}\frac{t^nQ^n}{n!} \\
    &= \sum_{n=1}^\infty \frac{n t^{n-1} Q^n}{n!} \\
    &= \sum_{n=1}^\infty \frac{t^{n-1} Q^{n-1}}{(n-1)!} Q \\
    &= e^{tQ} Q = Q e^{tQ} \qedhere
  \end{align*}
\end{proof}

\section{Exponential Distribution}
\begin{define}[Exponential Distribution] \label{def:exp-distri}
  A random variable $T: \Omega \to [0, \infty)$ has exponential distribution of parameter $\lambda (0 \leq \lambda < \infty)$ if
  \[\Pr[T > t] = e^{-\lambda t} \hspace{0.5cm} \mbox{for all $t \geq 0$}\].
  $\lambda$ is somethimes called ``rate'' of the exponential distribution.
  We write $T \sim E(\lambda)$ for short.
  If $\lambda > 0$, then $T$ has density function
  \[f_{T}(t) = \lambda e^{-\lambda t}\].
  The mean of $T$ is given by
  \[\mathbb{E}(T) = \int_{0}^\infty \Pr[T > t] \mathrm{d} t = 1/\lambda\]
\end{define}
\begin{theorem}[Memoryless]
  A random variable $T: \Omega \to [0, \infty)$ has an exponential distribution iff it has the following memoryless property:
  \[\Pr[T > s + t | T > s] = \Pr[T > t]\]
\end{theorem}
\begin{proof}
  $\Rightarrow$: if $T\sim E(\lambda)$, then
  \[\Pr[T > s + t | T > s] = \frac{\Pr[T > s + t]}{\Pr[T > s]} = \frac{e^{-\lambda(s + t)}}{e^{-\lambda s}} = e^{-\lambda t} = \Pr[T > t]\]

  $\Leftarrow$: if $T$ has memoryless property, then let $g(t) = \Pr[T > t]$.
  First, note that $g(1)$ is a constant, so there is some constant $\lambda$ such that $g(1) = e^{-\lambda}$.
  Then, for any rational number $p/q$, we have
  \[g(p/q) = g(1/q)^p = g(1)^{p/q}\]
  Finally, note that $g$ is decreasing, so for any real number $t$, we have $r \leq t \leq s$ ($r, s$ are rational numbers) and $g(r) \geq g(t) \geq g(s)$. Since $r$ and $s$ could be arbitrarily close to $t$, so we have $g(t) = e^{-\lambda t}$.
\end{proof}

\section{Possion Process}
\begin{define}[Possion Distribution]
  A random variable $X: \Omega \to \mathbb{Z}_{\geq 0}$ has Possion distribution of parameter $\lambda (0 \leq \lambda < \infty)$ if 
  \[\Pr[X = k] = \frac{\lambda^k e^{-\lambda}}{k!}\]
  We write $X\sim P(\lambda)$ for short.
\end{define}
\begin{fact}
    If $X\sim P(\lambda)$, then we have $\lambda = \mathbb{E}(X) = \mathrm{Var}(X)$.
\end{fact}
\begin{proof}
  \begin{align*}
    \mathbb{E}(X)
    &= \sum_{k = 0}^\infty \frac{\lambda^k e^{-\lambda}}{k!} \cdot k \\
    &= \sum_{k = 1}^\infty \lambda \cdot \frac{\lambda^{k-1}e^{-\lambda}}{(k-1)!} \\
    &= \lambda \sum_{k=0}^\infty \frac{\lambda^k e^{-\lambda}}{k!} = \lambda
  \end{align*}
  \begin{align*}
    \mathbb{E}(X^2)
    &= \sum_{k=0}^\infty \frac{\lambda^k e^{-\lambda}}{k!} \cdot k^2 \\
    &= \lambda e^{-\lambda} \sum_{k=1}^\infty \frac{\lambda^{k-1} e^{-\lambda}}{(k-1)!}\cdot k \\
    &= \lambda e^{-\lambda} \left[(k-1) \sum_{k=1}^\infty \frac{\lambda^{k-1} e^{-\lambda}}{(k-1)!} + \sum_{k=1}^\infty \frac{\lambda^{k-1} e^{-\lambda}}{(k-1)!} \right]\\
    &= \lambda e^{-\lambda} (\lambda e^\lambda + e^\lambda) \\
    &= \lambda^2 + \lambda \qedhere 
  \end{align*}
\end{proof}

Now we can define Possion process as follow.
\begin{define}[Possion Process]
  Let $T_1, T_2, \cdots$ be i.i.d. exponential random varibles of rate $r$ (see \ref{def:exp-distri}).
  Let $S_k := \sum_{i=1}^k T_i$ for $k\geq 1$.
  Let $X_t := k$ for $S_k \leq t < S_{k+1}$.
  Then, we say $(X_t)_{t\geq 0}$ is a Possion Process of rate $r$.
\end{define}
\begin{theorem}[Memoryless (Markov Property)] \label{thm:possion-memoryless}
  If $X_t$ is a Possion process of rate $r$, then for some constant $s > 0$, $(X_{s+t} - X_s)_{t\geq 0}$ is also a possion process of rate $r$.
\end{theorem}
\begin{proof}
  \begin{center}
    \includegraphics[width=0.8\textwidth]{markov-property-possion-process.png}
  \end{center}
\end{proof}

\begin{lemma}
  If $X_t$ is a Possion process of reate $r$, and let $S_k = \sum_{i=1}^k T_i$, then $S_k$ has a gamma distribution with shape parameter $k$ and rate parameter $r$, i.e. its density function is
  \[f_k(s) = \frac{r^ks^{k-1}e^{-r s}}{(k-1)!}\].
\end{lemma}
\begin{proof}
  We prove this by induction.
  As a base case, we have
  \begin{align*}
    f_2(s)
    &= \int_{-\infty}^\infty f_{T_1}(t) f_{T_2}(s - t) \mathrm{d} t \\
    &= \int_{0}^s r e^{-rt} r e^{-r(s-t)}\mathrm{d} t \\
    &= \int_{0}^s r^2 e^{-rs} \mathrm{d} t \\
    &= r^2 e^{-rs} s
  \end{align*}
  For the inductive case, we have:
  \begin{align*}
    f_{k+1}(s)
    &= \int_0^s f_k(t)f_{T_{k+1}}(s-t)\mathrm{d} t \\
    &= \int_0^s \frac{r^kt^{k-1}e^{-rt}}{(k-1)!} re^{-r(s-t)} \mathrm{d} t \\
    &= \int_0^s \frac{r^{k+1}t^{k-1}e^{-rs}}{(k-1)!} \mathrm{d} t \\
    &= \int_0^s \frac{r^{k+1}e^{-rs}}{k!} \mathrm{d} t^k \\
    &= \frac{r^{k+1}s^ke^{-r s}}{k!} \qedhere
  \end{align*}
\end{proof}

\begin{theorem}[\cite{levin2017markov}, Exercise 20.3] \label{thm:possion-process-to-possion-distri}
  If $(X_t)_{t\geq 0}$ is a Possion process of rate $r$, then for any $t, s \geq 0$,
  $X_{s+t} - X_s$ forms a Possion distribution with parameter $rt$
\end{theorem}
\begin{proof}
  Due to Theorem \ref{thm:possion-memoryless} we only need to show that for a fixed $t$, $X_t - X_0 = X_t$ forms a Possion distribution. Assume that $S_k = \sum_{i=1}^k T_i$.
  
  We prove this by induction.

  For the base case, we have $\Pr[X_t = 0] = \Pr[S_1 > t] = \Pr[T_1 > t] = e^{-rt}$. 
  For the inductive case, we have
  \begin{align*}
    \Pr[X_t = k]
    &= \Pr[S_k \leq t < S_{k+1}] = \Pr[S_k \leq t < S_k + T_{k+1}] \\
    &= \int_0^t \Pr[S_k = s] \cdot \Pr[T_{k+1} > t - s] \\
    &= \int_0^t f_k(s)\mathrm{d} s \cdot \Pr[T_{k+1} > t - s] \\
    &= \int_0^t \frac{r^ks^{k-1}e^{-rs}}{(k-1)!}\mathrm{d}s \cdot e^{-r(t - s)} \\
    &= \int_0^t \frac{r^ks^{k-1}e^{-rt}}{(k-1)!}\mathrm{d}s \\
    &= \frac{(rt)^ke^{-rt}}{k!} \qedhere
  \end{align*}
\end{proof}

\section{Construct CTMC}
There are mainly two different but equivalent way to construct a countinuous time Markov chain (CTMC) by using Possion clock. We give details about them one by one.
\begin{define}[Possion Clock]
  A Possion clock of rate $r$ is a clock that rings at time $T$, where $T$ is an exponential random variable with rate $r$.
\end{define}

\subsection{CTMC with a global Possion Clock}
\begin{define}[CTMC in \cite{levin2017markov}] \label{def:CTMC-LP17}
  For a discrete Markov chain $P$ and a Possion clock with rate $r$, we could make each transition of $P$ happens only in the moment where the clock rings. And thus get a CTMC $(X_t)_{t \geq 0}$.
\end{define}

Let $N_t$ denotes the number of ringings made by the clock.
By Theorem \ref{thm:possion-process-to-possion-distri}, we know that once $t$ is fixed to some constant, $N_t$ is a random variable that has Possion distribution with rate $rt$.
And note that
\[\Pr[X_t = y | N_t = k \land X_0 = x] = P^k(x, y)\].
So we have
\begin{align*}
  \tilde{P}^t(x, y)
  &= \sum_{k=0}^\infty\Pr[X_t = y | N_t = k \land X_0 = x]\cdot\Pr[N_t = k] \\
  &= \sum_{k=0}^\infty P^k(x, y) \cdot \frac{(rt)^ke^{-rt}}{k!} \\
  &= e^{-rt} \sum_{k=0}^\infty \frac{(rt)^kP^k(x,y)}{k!} \\
  &= e^{-rt} e^{rtP} \\
  &= e^{rt(P-I)}
\end{align*}
In \cite{levin2017markov}, they also denote $\tilde{P}^t$ as $H_t$ (called \textbf{\textit{heat kernel}}).

\subsection{CTMC with Possion Clocks on Each States}
\begin{define}[CTMC in \cite{norris1998markov}]
  For a discrete Markov chain $P$, i.e. $(Y_t)_{t\geq 0}$, before its $t$-th transition the current states, we put a Possion clock on each states of $P$ with different rate. Usually, we have a rate matrix $Q = P - I$. Then, $Y_t$ is determined by the first ringing.
\end{define}

The following theorem implies that the two definitions we given above are equivalent (some factors may not equal).
\begin{theorem}[\cite{norris1998markov}, Theorem 2.3.3.]
  Let $I$ be a countable set and let $T_k$, $k\in I$, be independet random variables with $T_k \sim E(q_k)$ and $0 < q := \sum_{k\in I} q_k < \infty$. Set $T = \inf_k T_k$. Then this infimum is attained at a unique random value $K$ of $k$, with probability $1$. Moreover, $T$ and $K$ are independent, with $T\sim E(q)$ and $\Pr[K = k] = q_k / q$.
\end{theorem}
\begin{proof}
  Set $K = k$ if $T_k < T_j$ for all $j\not= k$, otherwise let $K$ be undefined.
  Then
  \begin{align*}
    \Pr[K = k \land T \geq t]
    &= \Pr[T_k \geq t \land T_j > T_k, \forall j\not= k] \\
    &= \int_t^\infty q_ke^{-q_k s}\Pr[T_j > s, \forall j\not= k]\mathrm{d}s \\
    &= \int_t^\infty q_ke^{-q_k s}\prod_{j\not=k} e^{-q_j s} \mathrm{d} s\\
    &= \int_t^\infty q_k e^{-q s} \mathrm{d} s = \frac{q_k}{q}e^{-q t} 
  \end{align*}
  Hence $\Pr[K = k \mbox{ for some $k$}] = 1$ and $T$ and $K$ have the claimed joint distribution.
\end{proof}

\section{Different Mixing Times}
\begin{define}[$\ell^p$ distance]
  For any function $f:\Omega \to \mathbb{R}$, we have:
  \[\parallel f \parallel_{p, \pi} := \left(\sum_{x\in\Omega}\pi(x) |f(x)|^p\right)^{1/p}\]
\end{define}

\begin{define}[An Entropy-like Measure]
  \[\mathrm{Ent}_\pi(f) := \mathbb{E}_\pi[f \log f] - (\mathbb{E}_\pi f) \log \mathbb{E}_\pi f\]
  Note that, if $\mathbb{E}_\pi f = 1$, we have
  \[\mathrm{Ent}_\pi(f) = \mathbb{E}_\pi[f \log f]\]
\end{define}

There are many ways to measure the distance between $P^t(x, \cdot)$ and $\pi$.

\begin{define}
  For a discrete Markov chain $P$, let $k^x_t(y) := P^t(x,y)/\pi(y)$.
\end{define}

\begin{fact}
  \[\parallel P^t(x, \cdot) - \pi \parallel_{TV} = \frac{1}{2}\parallel k^x_t - 1 \parallel_{1,\pi}\]
\end{fact}

\begin{fact}
  \[\mathrm{Var}_\pi(k^x_t) = \parallel k^x_t - 1 \parallel_{2, \pi}\]
\end{fact}

\begin{fact}
  \[D(P^t(x, \cdot) \parallel \pi) =\sum_{y\in \Omega}\pi(y)\frac{P^t(x, y)}{\pi(y)}\log \frac{P^t(x,y)}{\pi(y)} = \mathrm{Ent}_\pi (k^x_t)\]
\end{fact}

\begin{define}[Some Mixing Times]
  \begin{align*}
    \tau(\varepsilon) &= \min\{n: \forall x\in \Omega, \parallel p^n(x,\cdot) - \pi \parallel_{TV} \leq \varepsilon\} \\
    \tau_D(\varepsilon) &= \min\{n: \forall x\in \Omega, D(p^n(x,\cdot) \parallel \pi) \leq \varepsilon\} \\
    \tau_2(\varepsilon) &= \min\{n: \forall x\in \Omega, \parallel p^n(x,\cdot) - \pi \parallel_{2, \pi} \leq \varepsilon\} 
  \end{align*}
\end{define}

\begin{fact}[\cite{levin2017markov}, Exercise 4.5]
  \[\parallel P^t(x, \cdot) - \pi \parallel_{TV} \leq \mathrm{Var}_\pi(k^x_t)\],
  and thus $\tau_2(\varepsilon) \geq \tau(\varepsilon)$.
\end{fact}

\begin{fact}[\cite{jerrum2005integrating}, Section 9.7; Pinsker's Ineq]
  \[2\parallel P^t(x,\cdot) - \pi \parallel^2_{TV} \leq D(P^t(x, \cdot) \parallel \pi) = \mathrm{Var}_\pi(k^x_t)\]
\end{fact}

\section{Bounds in Continuous Time}
See Section 1.2 of \cite{montenegro2006mathematical} for more details.

\clearpage
\bibliographystyle{alpha}
\bibliography{note}

\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End: