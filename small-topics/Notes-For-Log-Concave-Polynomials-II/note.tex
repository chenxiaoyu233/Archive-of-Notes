\documentclass{article}
\usepackage{geometry}
\geometry{}
\usepackage{geometry}
\geometry{a4paper, scale = 0.8}

\usepackage{ctex}
\usepackage{amsmath, amssymb, amsthm}
\everymath{\displaystyle}
\newtheorem{define}{Define}
\newtheorem{fact}{Fact}
\newtheorem*{fact*}{Fact}
\newtheorem{property}{Property}
\newtheorem{theorem}{Theorem}
\newtheorem*{theorem*}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem*{lemma*}{Lemma}
\newtheorem{claim}{Claim}
\newtheorem{corollary}{Corollary}

\usepackage{tabularx}
\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=cyan,
}

\title{Notes for Log-Concave Polynomials II}
\author{Xiaoyu Chen}
\date{}

% 简化书写
\def\<{\langle}
\def\>{\rangle}
\def\tr{\mathrm{tr}}
\def\diag{\mathrm{diag}}
\def\dim{\mathrm{dim}\,}

\begin{document}
\maketitle
\section{背景知识}
这篇文章中涉及了大量我以往不太了解的技术, 这里集中做一个整理. 为了节约时间, 我会略过所有我能马上反应过来的推导. 并且, 为了简化工作, 尽量省略了所有的交叉引用, 如果发现后面有地方的证明看不过去, 可以去翻前面的结论.
\subsection{线性代数}
\subsubsection{Schur Product Theorem}
\begin{fact}
  $\mathrm{tr}(AB^T) = \sum_{i, j=1}^n A_{ij}B_{ij}$
\end{fact}
\begin{proof}[proof]
  $\mathrm{tr}(AB^T) = \sum_{i} (AB^T)_{ii} 
  = \sum_{i=1}^n\sum_{j=1}^nA_{ij}B^T_{ji} 
  = \sum_{i,j=1}^nA_{ij}B_{ij}  $
\end{proof}

\begin{fact}
  \label{claim_xAoBx}
  $x^*(A\circ B)y = \tr((\diag\ \overline{x}) A (\diag\ y) B^T)$
\end{fact}
\begin{proof}[proof]
  首先可以发现:
  $[(\diag\ \overline{x})A]_{ij} = \sum_{k=1}^n(\diag\ \overline{x})_{ik}A_{kj} = \overline{x}_i A_{ij}$.
  同理有: $[B(\diag\ y)]_{ij} = B_{ij}y_j$.
  所以:
  \begin{align*}
    \tr((\diag\ \overline{x}) A (\diag\ y) B^T) &= \sum_{i, j=1}^n [(\diag\ \overline{x})A]_{ij}[B(\diag\ y)]_{ij} \\
                                                &= \sum_{i, j=1}^n \overline{x}_iA_{ij}B_{ij}y_j \\
                                                &= \sum_{i, j=1}^n \overline{x}_i(A\circ B)_{ij}y_j \\
                                                &= x^*(A\circ B)y \qedhere
  \end{align*}
\end{proof}

\begin{fact}
  如果矩阵 $B$ 是一个Hermitian 阵, 则 $B^{1/2}$ 存在.
\end{fact}
\begin{proof}[proof]
  因为 $B$ 是一个Hermitian 阵, 所以存在 $P$, 使得 $B = P\Lambda P^{-1}$($\Lambda$是一个对角阵).
  这个时候显然, $B^{1/2} = P\Lambda^{1/2} P^{-1}$.
\end{proof}

\begin{fact}
  $\tr(ABC) = \tr(BCA)$
\end{fact}
\begin{proof}[proof]
  \begin{align*}
    \tr(ABC) = \sum_i [ABC]_{ii} 
             = \sum_{i, j, k} A_{ij}B_{jk}C_{ki} 
             = \sum_{i, j, k} C_{kj}A_{ij}B_{jk} 
            = \tr(CAB)
  \end{align*}
\end{proof}

\begin{theorem}[Schur Product Theorem]
  如果 $A, B \in \mathbb{R}^{n\times n}$ 都是半正定的, 则 $A\circ B$ 也是半正定的.
  \vspace{0.3cm}
  
  $\triangle$ 注: $A \circ B_{ij} = A_{ij}B_{ij}$ 
  
  $\triangle$ 注: 当人们在讨论正定或者半正定的矩阵的时候, 一般都默认这些矩阵是 Hermitian 阵 (这个约定可以在
  \href{https://en.wikipedia.org/wiki/Definiteness_of_a_matrix}{wiki}
  上关于正定矩阵的定义发现)
\end{theorem}
\begin{proof}[proof]
  \begin{align*}
    x^*(A\circ B)x &= \tr((\diag\ \overline{x})A(\diag\ x)B^T) \\
    &= \tr((\diag\ \overline{x})A(\diag\ x)\overline{B}), \hspace{0.4cm} \mbox{$B$ is hermitian} \\
                   &= \tr(\overline{B}^{1/2}(\diag\ \overline{x})A(\diag\ x)\overline{B}^{1/2}) \\
    &= \tr(C^*AC), \hspace{0.5cm} \mbox{其中}C = (\diag\ x)\overline{B}^{1/2}
  \end{align*}
  因为 $A$ 半正定, 所以令 $y = Cx$ 可得 $y^*Ay \geq 0$.
  这也说明 $C^*AC$ 半正定, 所以 $\tr(C^*AC) \geq 0$.
  所以 $x^*(A\circ B)x \geq 0$, 所以 $A\circ B$ 半正定.
\end{proof}

\subsubsection{Positive Matrix}
下面来学习一些关于 positive matrix 的结果.
\begin{define}[positive matrix]
  如果对于一个矩阵 $A$, 有 $A_{ij} > 0, \forall i, j$, 则说 $A$ 是一个 positive matrix.

  $\triangle$ 注: 如果$A$是正项矩阵, 计作 $A > 0$. 如果  $x$ 是正项向量, 计作 $x > 0$.
\end{define}
\begin{define}[谱半径]
  矩阵$A$的谱半径定义为 $\rho(A) = \max\{|\lambda|: \lambda \mbox{ 是$A$的一个特征值}\}$
\end{define}

\begin{define}
  令 $x$ 是一个向量, 则 $|x|$ 是一个向量, 且 $|x|_i = |x_i|$.

  $\triangle$ 注: 这里在复数域 $\mathbb{C}$ 上, 所以对于 $a\in\mathbb{C}$, $|a|$ 指的是 $a$ 的模长.
\end{define}

\begin{fact}
  对于 $\forall x\in \mathbb{C}^n$, 都 $\exists \theta \in \mathbb{R}$, 使得 $e^{-i\theta} x = |x|$.
\end{fact}
\begin{proof}
  这个结论是复数乘法的几何意义的一个应用.
  欧拉公式告诉我们, 两个复数相乘, 结果是模长相乘, 夹角相加.
  因为所有的复数都能被写成 $re^{i\theta}$的形式, 其中 $r$ 是模长, $\theta$ 是夹角.
  所以, 如果我们想知道一个复数 $x$ 的模长, 我们可以通过将其旋转到实轴正半轴来实现, 旋转的操作可以有某个单位长度的复数来做到, 具体来说, 如果 $x = re^{i\theta}$, 我们只需要给他乘上一个 $e^{-i\theta}$ 即可.
\end{proof}

\begin{fact}
  $x\in \mathbb{C}^n$, $|\sum_i x_i| \leq \sum_i |x_i|$.
  当$\exists \theta\in\mathbb{R}$, 使得 $e^{-i\theta}x = |x|$ 时取等.

  $\triangle$ 注: 这个式子可以看成绝对值不等式在复数域和高维向量空间的推广.
\end{fact}
\begin{proof}
  首先, 不难发现, $\sum_i x_i \in \mathbb{C}$.
  所以由上面的结论, 就 $\exists \theta$, 使得 $e^{-i\theta} (\sum_i x_i) = |\sum_i x_i|$.
  所以
  \begin{align*}
    |\sum_i x_i| &= e^{-i\theta}(\sum_i x_i) \\
    &= \mathrm{Re}\left[e^{-i\theta}(\sum_i x_i)\right] \\
    &= \sum_j \mathrm{Re}\left[e^{-i\theta} x_j\right]\\
    &\leq \sum_j |e^{-i\theta}x_j| \\
    &= \sum_j |x_j|
  \end{align*}
  从上面的式子, 不难发现, 这个式子如果要取到等号, 则 $\mathrm{Re}\left[e^{-i\theta}x_j\right] = |x_j|, \forall j$.
  这个时候显然就有 $e^{-i\theta}x_j = |x_j|, \forall j$.
  \underline{也就是说, 如果要取等号, 则有 $e^{-i\theta} x = |x|$.}
\end{proof}
\begin{fact}
  $|Ax| \leq |A|\,|x|$
\end{fact}
\begin{proof}
  \begin{align*}
    |Ax|_k &= |\sum_i A_{ki}x_i| \\
           &\leq \sum_i |A_{ki}x_i| \\
    &= \sum_i |A_{ki}|\,|x_i| = [|A|\,|x|]_k
  \end{align*}
\end{proof}

\begin{lemma}
  令 $A$ 是一个 positive matrix.
  $\lambda, x$ 是 $A$ 的一个特征值对, 且 $|\lambda| = \rho(A)$.
  则有 $|x| > 0$ 且有 $A|x| = \rho(A)|x|$, 且 $\exists \theta$ 使得 $e^{i\theta}x = |x|$.
\end{lemma}
\begin{proof}[proof]
  因为 $A > 0$, 所以直接就有 $z = A|x| > 0$.
  所以 \[z = A|x| = |A||x| \geq |Ax| = |\lambda x| = |\lambda| |x| = \rho(A)|x|\].
  即 $z = A|x| \geq \rho(A)|x|$. 这个时候不妨设 $y = z - \rho(A)|x| \geq 0$.

  (1) 如果 $y = 0$, 则 $\rho(A)|x| = A|x| > 0$, 所以$\rho(A) > 0, |x| > 0$.

  (2) 如果 $y > 0$, 则 $Ay = Az - \rho(A)z > 0$.
  所以 $Az > \rho(A)z$. 这和 $\rho(A)$ 的定义矛盾了.

  $\triangle$ 注: $\lambda_{\max} = \max_{x\not=0} \frac{x^*Ax}{x^*x}$,
  所以上面(2)中的现象会导致 $\frac{z^*Az}{z^*z} > \rho(A)$, 即 $\lambda_{\max} > \rho(A)$.
  
  \vspace{0.3cm}
  综合  (1), (2) 两点, 有 $A|x| = \rho(A)|x|$, 且 $\rho(A) > 0, |x| > 0$.
  从 $A|x| = \rho(A)|x|$, 可以知道
  \[|\sum_i A_{ki}x_i| = \sum_i |A_{ki}x_i|, \hspace{0.5cm} \forall k\]
  这时对于某个固定的$k$, 有 $\exists \theta\in\mathbb{R}$, 使得
  \[e^{-i\theta} A_{kj}x_j = |A_{kj}||x_j|, \hspace{0.5cm} \forall j\]
  因为 $A > 0$, 所以
  \[e^{-i\theta} x_j = |x_j|, \hspace{0.5cm} \forall j\]
  ,即 $e^{-i\theta} x = |x|$.
\end{proof}

\begin{lemma}
  $A \in \mathbb{R}^{n\times n}, A > 0$, 则有, $A$ 关于 $\rho(A)$ 的几何重数是 $1$.
\end{lemma}
\begin{proof}[proof]
  设有 $w, z\in\mathbb{C}^n$, 且有 $\substack{Aw = \rho(A)w\\ Az = \rho(A)z}$.
  由前面的结论可知:

  (1) $\exists \theta_1\in\mathbb{R}$ 使得 $p = e^{-i\theta_1}z > 0$, 且 $Ap = \rho(A)p$.

  (2) $\exists \theta_2\in\mathbb{R}$ 使得 $q = e^{-i\theta_2}w > 0$, 且 $Aq = \rho(A)q$.

  这个时候, 不妨设 $\beta = \min_{1\leq i\leq n} \frac{q_i}{p_i}$, $r = q - \beta p$.
  这时 $r \geq 0$ 且 $r$ 至少有一个地方为 $0$.

  如果 $r\not = 0$, 则
  \[ 0 < Ar = A(q - \beta p) = \rho(A)q - \beta\rho(A)p = \rho(A)r\]
  所以 $\rho(A) > 0, r > 0$, 这和 $r$ 上有一个地方为 $0$ 有矛盾.

  若果  $r = 0$, 则 $q = \beta p$.
  \begin{align*}
    e^{-i\theta_2}w &= \beta e^{-i\theta_1}z \\
    w &= \beta e^{-i(\theta_1 - \theta_2)}z
  \end{align*}
  故 $w$ 和 $z$ 只呈常数倍关系.
\end{proof}

将上面两个引理的结论收集起来, 就能得到下面定理.
\begin{theorem}[Perron-Frobenius Theorem]
  令 $A\in\mathbb{R}^{n\times n}, A > 0$.
  则 $A$ 有一个严格为正的特征值 $\lambda = \rho(A)$.
  且 $\lambda$ 对应的特征向量 $x > 0$.
  $A$ 关于 $\lambda$ 的几何重数是 $1$.
\end{theorem}

\subsubsection{特征值和子空间中的向量}
\begin{fact}
  对于一个矩阵 $A$, 我们可以将他的特征值从小打到排列:
  \[\lambda_{\min} = \lambda_1 \leq \lambda_2 \leq \cdots \leq \lambda_{n-1} \leq \lambda_n = \lambda_{\max}\]
\end{fact}

\begin{fact}
  对于 Hermitian 矩阵来说,  我们有 $n$ 个相互正交的特征向量
\end{fact}

\begin{theorem}[Rayleigh theorem]
  假设有 $1 \leq i_1 \leq \cdots \leq i_k \leq n$.
  且有 $x_{i_1}, x_{i_2}, \cdots, x_{i_k} \in \mathbb{C}^n$ 相互正交.
  且有 $Ax_{i_p} = \lambda_{i_p}x_{i_p}, p = 1, 2, \cdots, k$.
  然后令 $S = \mathrm{Span}\{x_{i_1}, x_{i_2}, \cdots x_{i_k}\}$ 则有:
  \begin{align*}
    \lambda_{i_1} &= \min_{\substack{x\not=0\\ x\in S}} \frac{x^*Ax}{x^*x} = \min_{\substack{x\in S\\ ||x||_2=1}} x^*Ax \\
    &\leq \max_{\substack{x\in S\\ ||x||_2 = 1}} x^*Ax = \max_{\substack{x\not=0\\x\in S}} \frac{x^*Ax}{x^*x} = \lambda_{i_k}
  \end{align*}
\end{theorem}
\begin{proof}
  对于 $\forall x\in S, ||x||_2 = 1$, 我们有:
  \[x = \alpha_1 x_{i_1} + \alpha_2x_{i_2} + \cdots + \alpha_kx_{i_k}\]
  且 
  \begin{align*}
    x^*x &= 1 \\
    1 & = \sum_{j=1}^k (\alpha_jx_{i_j})^*\alpha_jx_{i_j} \\
         &= \sum_{j=1}^k \overline{\alpha_j}\alpha_j x_{i_j}^* x_{i_j} \\
    &= |\alpha_1|^2 + |\alpha_2|^2 + \cdots + |\alpha_k|^2, \hspace{0.5cm} \mbox{这里不妨$x_{i_j}$ 都是单位长度向量}
  \end{align*}
  又有
  \begin{align*}
    x^*Ax  &= (\alpha_1x_{i_1} + \cdots + \alpha_kx_{i_k})^*(\alpha_1\lambda_{i_1}x_{i_1} + \cdots \alpha_k\lambda_{i_k}x_{i_k}) \\
    &= |\alpha_1|^2\lambda_{i_1} + \cdots + |\alpha_k|^2\lambda_{i_k}
  \end{align*}
  因为 $|\alpha_1|^2 + \cdots + |\alpha_k|^2 = 1$, 所以 $x^*Ax$ 是 $\lambda_{i_1}, \cdots, \lambda_{i_k}$ 的一个凸组合.
  所以, 当然有 $\lambda_{i_1} \leq x^*Ax \leq \lambda_{i_k}$, 并且刚好可以取到端点.
\end{proof}

\begin{lemma}[空间交]
  令 $S_1, S_2, \cdots, S_k$ 是 $V = \mathbb{C}^n$ 的子空间.\\ 
  如 $\delta = \dim S_1 + \dim S_2 + \cdots \dim S_k - (k-1)n \geq 1$, 
  则 $S_1 \cap S_2 \cap \cdots \cap S_k$ 中有$\delta$ 个相互正交的单位向量.
\end{lemma}
\begin{proof}
  先来考虑两个子空间的情形.
  对于任意两个子空间 $S_1, S_n \subset V$, 有
  \[\dim (S_1\cap S_2) + \dim (S_1 + S_2) = \dim S_1 + \dim S_2\]
  所以
  \begin{align*}
    \dim (S_1 \cap S_2) &= \dim S_1 + \dim S_2 - \dim (S_1 + S_2) \\
                        &\geq \dim S_1 + \dim S_2 - \dim V \\
                        &= \dim S_1 + \dim S_2 - n
  \end{align*} \\

  然后, 在这个基础上, 我们来使用归纳法.
  假设对于 $k$ 个子空间, 有:
  \[\dim (S_1 \cap \cdots \cap S_k) = \sum_{i=1}^k \dim S_i - (k - 1)n\]
  则 对于 $k+1$ 个子空间的情况, 我们有:
  \begin{align*}
    \dim(S_{k+1}\cap (S_1 \cap \cdots S_k)) + \dim (S_{k+1} + (S_1 \cap \cdots \cap S_k)) &= \dim (S_1\cap \cdots \cap S_k) + \dim S_{k+1} 
  \end{align*}
  所以 
  \begin{align*}
    \dim(S_1 \cap \cdots \cap S_{k+1}) &= \sum_{i=1}^k \dim S_i - (k - 1)n + \dim S_{k+1} - \dim(S_{k+1}  + (S_1 \cap \cdots \cap S_k)) \\
    &\leq \sum_{i=1}^{k+1} \dim S_i - (k - 1)n  - \dim V \\
    &= \sum_{i=1}^{k+1} \dim S_i - (k - 1)n - n \\
    &= \sum_{i=1}^{k+1} \dim S_i - kn
  \end{align*}
  所以这里有 $\dim (S_1 \cap \cdots S_k) \geq \delta$.
\end{proof}

\begin{theorem}[Courant-Fischer Theorem]
  $A$ 是 Hermitian 阵, 则
  \[\lambda_k = \min_{\substack{S\subset V\\ \dim S = k}}\max_{\substack{x\not=0\\x\in S}}\frac{x^*Ax}{x^*x}\]
\end{theorem}
\begin{proof}[proof]
  由题, $S$ 是任意 $k$ 维空间.
  令 $S' = \mathrm{Span}\{x_k, \cdots, x_n\}$.
  则 $\dim S + \dim S' = k + (n - k + 1) = n + 1$.
  所以 $\delta \geq 1$, 故 $\{x\not=0 \land x\in S\cap S'\}$ 非空.
  这时有:
  \begin{align*}
    \sup_{\substack{x\not=0\\x\in S}}\frac{x^*Ax}{x^*x} &\geq \sup_{\substack{x\not=0\\x\in S\cap S'}}\frac{x^*Ax}{x^*x} \geq \inf_{\substack{x\not=0\\x\in S\cap S'}} \frac{x^*Ax}{x^*x} \\
    &\geq \inf_{\substack{x\not=0\\x\in S'}}\frac{x^*Ax}{x^*x} \geq \min_{\substack{x\not=0\\x\in S'}}\frac{x^*Ax}{x^*x} = \lambda_k
  \end{align*}
  总结一下就是
  \[\sup_{\substack{x\not=0\\x\in S}} \frac{x^*Ax}{x^*x} \geq \lambda_k, \hspace{0.5cm} \forall S\subset \mathbb{C}^n\]
  所以自然就有:
  \[\inf_{\substack{S\subset \mathbb{C}^n\\ \dim S = k}}\sup_{\substack{x\not=0\\x\in S}} \frac{x^*Ax}{x^*x} \geq \lambda_k\]
  下面来研究这个等号是否一定能够取到:
  令这里的 $x_1, x_2, \cdots, x_n$ 分别是 $\lambda_1, \cdots, \lambda_n$ 对应的特征向量, 且相互正交.
  所以取 $S = \mathrm{Span}\{x_1, \cdots, x_k\}$ 即可取到等号, 故
  \[\inf_{\substack{S\subset \mathbb{C}^n\\ \dim S = k}}\sup_{\substack{x\not=0\\x\in S}} \frac{x^*Ax}{x^*x} = \lambda_k\]
\end{proof}

\subsubsection{柯西交错定理}
\begin{theorem}[Weyl Theorem]
  将 Hermitian 阵 $A, B, A+B$ 的特征值都从小到大排序, 然后我们有:
  \[\lambda_i(A + B) \leq \lambda_{i+j}(A) + \lambda_{n-j}(B), \hspace{0.5cm} \substack{j\in[0, i-1]\\i\in[1,n]}\hspace{0.5cm} (1)\]
  对称的, 我们还有:
  \[\lambda_{i-j+1}(A) + \lambda_j(B) \leq \lambda_i (A + B), \hspace{0.5cm} \substack{j\in[1, i]\\ i\in[1,n]} \hspace{0.5cm} (2)\]
\end{theorem}
\begin{proof}
  这个定理看上去十分复杂, 但是思路很好理解.
  先来证明(1), 设
  
  \begin{tabular}{ll}
    $S_1 = \{x_i, \cdots, x_n\}$, & $(A+B)x_i = \lambda_i(A+B)x_i$. \\
    $S_2 = \{y_1, \cdots, y_{i+j}\}$, & $Ay_i = \lambda_i(A)y_i$. \\
    $S_3 = \{z_1, \cdots, z_{n-j}\}$, & $Bz_i = \lambda_i(B)z_i$.
  \end{tabular}

  很容易发现 $\delta + 2n = \dim S_1 + \dim S_2 + \dim S_3 = (i+j) + (n-j) + (n-i+1) = 2n+1$.
  故 $\delta \geq 1$, 所以 $S_1 \cap S_2 \cap S_3$ 非空, 故对 $x\in S_1\cap S_2\cap S_3$, 有:
  \begin{align*}
    \lambda_i(A+B) \underbrace{\leq}_{S_1} x^*(A+B)x &= x^*Ax + x^*Bx \\
    &\leq \underbrace{\lambda_{i+j}(A)}_{S_2} + \underbrace{\lambda_{n-j}(B)}_{S_3}
  \end{align*}
  同理即可证明 (2).
\end{proof}

\begin{corollary}
  令 $A, B$ 为 Hermitian 阵, 并设 $B$ 有 $\pi$ 个正特征值, $\nu$ 个负特征值.
  则显然有 $\substack{\lambda_{n-\pi}(B) \leq 0 \\ \lambda_{\nu+1}(B) \geq 0}$.故:

  [1]
  \begin{align*}
    \lambda_i(A + B) &\leq \lambda_{i+\pi}(A) + \underbrace{\lambda_{n-\pi}(B)}_{\leq 0} \\
    &\leq \lambda_{i+\pi}(A), \hspace{0.5cm} i\in[1, n-\pi]
  \end{align*}

  [2]
  \begin{align*}
    \lambda_{i-\nu}(A) + \underbrace{\lambda_{1+\nu}(B)}_{\geq 0} &\leq \lambda_i(A+B) \\
    \lambda_{i-\nu}(A) &\leq \lambda_i(A+B), \hspace{0.5cm} i\in[\nu+1, n]
  \end{align*}
\end{corollary}

\begin{fact}
  $\forall z\in \mathbb{C}^n$, 是非$0$向量, 且 $n\geq 2$, 则有:
  $\lambda_{n-1}(zz^*) = 0 = \lambda_2(zz^*)$
\end{fact}
\begin{proof}[proof]
  考虑两个特征值对, $\substack{\lambda_1 \not= 0, x\not= 0 \\ \lambda_2 \not=0, y\not=0}$.
  因为 $zz^*x = \lambda_1x$, 所以 $z^*x \not= 0$, 同理可得 $z^*y\not=0$.
  
  于是就有:
  \begin{align*}
    0 \not= (z^*y)^*(z^*x) = y^*zz^*x = \lambda_1 y^*x \not= 0 \\
    0 \not= (z^*x)^*(z^*y) = x^*zz^*y = \lambda_2 x^*y \not= 0 
  \end{align*}
  显然, $(z^*y)^*(z^*x) = \overline{(z^*x)^*(z^*y)}$, 所以$\lambda_1y^*x = \overline{\lambda_2x^*y}$
  所以 $\lambda_1 = \overline{\lambda_2}$.
  又因为 $zz^*$ 是Hermitian 阵, 所以其特征值都是实数, 所以 $\lambda_1 = \lambda_2$.
\end{proof}
\begin{fact}
  $\forall v\in\mathbb{C}^n$, 是非0向量, 且 $n \geq 2$, 则有 $vv^*$ 的非$0$ 特征向量 $>0$.
\end{fact}
\begin{proof}[proof]
  设$\lambda \not=0, x\not=0$ 为 $vv^*$ 的非$0$特征值对. 则,
  \[0 < \sum_{i=1}^n |[v^*x]_i|^2 = (v^*x)^*(v^*x) = x^*vv^*x = \lambda x^*x = \lambda \sum_{i=1}^n |x_i|^2\].
  故 $\lambda > 0$.
\end{proof}
\begin{theorem}[Cauchy's Interlacing Theorem]
  $n \geq 2$, $A$ 是 Hermitian 阵, 那么: 对于 $B = A + vv^*$, 有:
  \[\lambda_1(A) \leq \lambda_1(B) \leq \cdots \leq \lambda_{n}(A) \leq \lambda_n(B)\]
\end{theorem}
\begin{proof}[proof]
  因为 $vv^*$ 有最多1个正特征值, 且其他的特征值都为0, 所以:
  \begin{align*}
    \lambda_i(A + vv^*) &\leq \lambda_{i+1}(A) + \underbrace{\lambda_{n-1}(vv^*)}_{\leq 0} \\
    \lambda_i(B) &\leq \lambda_{i+1}(A)
  \end{align*}
  \begin{align*}
    \lambda_{i-1}(A) + \underbrace{\lambda_{i+1}(vv^*)}_{\geq 0} &\leq \lambda_i(A + vv^*) \\
    \lambda_{i-1}(A) &\leq \lambda_i(B)
  \end{align*}
\end{proof}

\subsubsection{文章中将会用到的一些结论}
\begin{define}[为正的特征值个数]
  文章中说的特征值个数, 是真的个数, 如果一个矩阵 $A \in \mathbb{R}^{n\times n}$, 有两个特征值, $\lambda_1, \lambda_2$, 这个时候, 是算两个特征值的.

  $\triangle$ 注: 反正最有要保证 $A$ 有 $n$ 个特征值. i.e. 我们对于不平行的特征向量构成的特征值, 我们都算作不同的特征值.
\end{define}
\begin{fact}
  \label{proof_PSD}
  如果 $A \in \mathbb{R}^{n\times n}$ 最多只有一个正特征值,
  则存在 $B \in \mathbb{R}^{n\times n}, v \in \mathbb{R}^n$,
  使得 $A = B + vv^*$, 且 $B$ 是一个半负定矩阵.
\end{fact}
\begin{proof}[proof]
  根据 [Courant-Fischer Theorem] 我们知道, 存在一个 $x \in \mathbb{R}^n$, 使得 $x^*Ax = \lambda_{\max}x^*x$, 且 $x^*x = 1$.

  这个时候, 我们去找一个 $v\in\mathbb{R}^n$, 使得 $v^*x = \lambda_{\max}^{1/2}$.
  (可以直接取 $v = \lambda_{\max}^{1/2}x$)
  显然, 这样的 $v$ 存在很多个, 我们任意取一个.

  这个时候, 令 $B = A - vv^*$, 就有:
  \begin{align*}
    x^*Bx  &= x^*Ax - x^*vv^*x \\
    &= \lambda_{\max} - (v^*x)^*(v^*x) \\
    &= \lambda_{\max} - \lambda_{\max}^{1/2}\lambda_{\max}^{1/2} \\
    &= 0
  \end{align*}
  因为 $A$ 只有一个为正的特征向量, $vv^*$ 的所有特征向量非负.
  所以, 对于 $\forall y\in\mathbb{R}^n$ 且 $y$ 与 $x$ 不平行, 都有
  $\substack{y^*Ay\leq 0 \\ y^*vv^*y = 0}$, 所以 $y^*By \leq 0$, 所以 $B$ 是半负定矩阵.
\end{proof}

\begin{define}[半定矩阵记法]
  文章中, 通常用 $A\preccurlyeq 0$ 表示 $A$ 是负定的.
  对称的, 一般用 $A\succcurlyeq 0$ 表示 $A$ 是正定的.

  $\triangle$ 注: $A\preccurlyeq B$ 表示 $A - B \preccurlyeq 0$.
\end{define}

\begin{lemma*}[2.3]
  令 $A \in \mathbb{R}^{n\times n}$ 是一个对称阵.
  令 $P \in \mathbb{R}^{m\times n}$.
  如果 $A$ 最多有一个正特征值, 则 $PAP^T$ 最多有一个正特征值.
\end{lemma*}
\begin{proof}[proof]
  由前面的结论知道, 我们可以令 $A = B + vv^*$, 其中 $B\preccurlyeq 0$.
  然后就有:
  \begin{align*}
    PAP^T &= PBP^T + \underbrace{Pvv^*P^T}_{w := Pv} \\
    x^*PAP^Tx &= x^*PBP^Tx + xww^*x \\
    &= (P^Tx)^*B(P^Tx) + xww^*x
  \end{align*}
  因为 $B\preccurlyeq 0$, 且 $ww^*$ 最多只有一个正特征值, 所以$PAP^T$ 最多只有一个正特征值.
\end{proof}
\begin{fact*}[2.4]
  令 $A\in\mathbb{R}^{n\times k}, B\in\mathbb{R}^{k\times n}$.
  $AB$ 的非0特征值等于 $BA$ 的非0特征值.且具有相同的代数重数.
\end{fact*}
\begin{proof}[proof]
  \[
    \underbrace{
      \left[
      \begin{array}{cc}
        I_{n \times n} & -A \\
        0 & I_{k\times k}
      \end{array}
    \right]
    }_{C_1}
    \underbrace{
    \left[
      \begin{array}{cc}
        AB & 0 \\
        B  & 0_{k\times k}
      \end{array}
    \right]
    }_{S_1}
    \underbrace{
    \left[
      \begin{array}{cc}
        I_{n \times n} & A \\
        0 & I_{k\times k}
      \end{array}
    \right]
    }_{C_2}
    =
    \underbrace{
    \left[
      \begin{array}{cc}
        0_{n\times n} & 0 \\
        B  & BA
      \end{array}
    \right]
    }_{S_2}
  \]
  不难发现, $C_1C_2 = I$, 所以 $S_1$ 和 $S_2$ 相似.
  所以 $S_1, S_2$ 有相同的特征值.

  考虑 $S_1$ 的特征方程:
  \begin{align*}
    P(S_1) &= 0 \\
    \mathrm{det}(S_1 - \lambda I) &= 0 \\
    \mathrm{det}\left(\left[
    \begin{array}{cc}
      AB - \lambda I_{n\times n}& 0 \\
      B & -\lambda I_{k\times k}
    \end{array}
    \right]\right) &= 0
  \end{align*}
  计算行列式时, 从右下角按列展开, 不难发现, $S_1$ 的 特征值等于 $AB$的特征值加上$k$个$0$. 

  同理可得, $S_2$ 的特征值等于 $BA$ 的特征值加上 $n$个 $0$.

  又因为 $S_1$ 和 $S_2$ 的特征值相同, 所以不难发现, $AB$ 和 $BA$ 的非0特征值相同,
  且具有相同的代数重数.
\end{proof}

\begin{lemma*}[2.5]
  $A\in\mathbb{R}^{n\times n}$ 是一个对称阵, 且最多只有一个正特征值.
  那么对任意 $B\in\mathbb{R}^{n\times n}, B\succcurlyeq 0$, 都有 $BA$ 最多有一个正特征值.
\end{lemma*}
\begin{proof}[proof]
  因为 $B$ 半正定, 所以 $B$ 实对称. 所以 $\exists P$ 使得 $B = P\Lambda P^T$
  ($\Lambda$为对角阵) 所以
  \begin{align*}
    B &= P\Lambda P^T \\
      &= P\Lambda^{1/2}\Lambda^{1/2}P^T \\
    &= (\Lambda^{1/2}P^T)^T(\Lambda^{1/2}P^T)
  \end{align*}
  所以, $B$ 可以被写成 $C^TC$ 的形式.

  故:
  \begin{align*}
    BA &= C^TCA \\
    &= C^T(CA)
  \end{align*}
  根据 Fact (2.4) 可以发现, $C^T(CA)$ 和 $CAC^T$ 具有相同的非0特征值.
  又因为 $A$ 最多又一个正特征值, 所以 $CAC^T$ 最多有一个正特征值, 所以 $BA$ 最多有一个正特征值.
\end{proof}

\begin{lemma*}[2.6]
  令 $A \in \mathbb{R}^{n\times n}$ 为一个实对称矩阵.
  且 $A \geq 0$, $A$  最多有一个正特征值.
  然后令 $w \in \mathbb{R}^n, w_i = \sum_{j=1}^n A_{ij}$ 且 $w_i > 0, \forall i$.
  则
  \[A \preccurlyeq \frac{ww^T}{\sum_i w_i}\]
\end{lemma*}
\begin{proof}[proof]
  令 $W = \diag\,w$.
  根据 Lemma (2.3) $\mathcal{A} = W^{-1/2}AW^{-1/2}$ 最多只有一个正特征值.
  (这里$\mathcal{A}$成立的原因是 $W$ 中的元素都是正数)
  然后令 $\sqrt{w} \in \mathbb{R}^n: \sqrt{w}(i) = \sqrt{w_i}$.
  则
  \begin{align*}
    \mathcal{A}\sqrt{w} &= W^{-1/2}AW^{-1/2}\sqrt{w} \\
                        &= W^{-1/2}A\,\mathbf{1} \\
                        &= W^{-1/2}w \\
                        &= \sqrt{w}
  \end{align*}
  所以 $\sqrt{w}$ 取到了 $\mathcal{A}$ 唯一的正特征值, i.e. 1.

  又有:
  \begin{align*}
    \sqrt{w}^T(\mathcal{A} - \frac{\sqrt{w}\sqrt{w}^T}{||\sqrt||^2})\sqrt{w} = \sqrt{w}^T\mathcal{A}\sqrt{w} - \sqrt{w}^T\sqrt{w} = 0
  \end{align*}
  所以 $\mathcal{A} \preccurlyeq \frac{\sqrt{w}\sqrt{w}^T}{||\sqrt{w}||^2} = \frac{\sqrt{w}\sqrt{w}^T}{\sum_i w_i}$ [这里可以用和 Fact \ref{proof_PSD} 相同的语句来证明].

  更进一步, 因为 $\mathcal{A}\preccurlyeq \frac{\sqrt{w}\sqrt{w}^T}{\sum_i w_i}$, 所以 $\forall x\in\mathbb{R}^n, x^Tx = 1$, 都有:
    \[x^T(\mathcal{A} - \frac{\sqrt{w}\sqrt{w}^T}{\sum_i w_i})x \leq 0\]
    因为 $W^{1/2}x \in \mathbb{R}^n$, 故
    \begin{align*}
      (W^{1/2}x)^T(\mathcal{A} - \frac{\sqrt{w}\sqrt{w}^T}{\sum_i w_i})(W^{1/2}x) &\leq 0 \\
      x^TW^{1/2}(\mathcal{A} - \frac{\sqrt{w}\sqrt{w}^T}{\sum_i w_i})W^{1/2}x &\leq 0 \\
      x^T(W^{1/2}\mathcal{A}W^{1/2} - \frac{W^{1/2}\sqrt{w}\sqrt{w}^TW^{1/2}}{\sum_i w_i})W^{1/2}x &\leq 0 \\
      x^T(A - \frac{ww^T}{\sum_i w_i})W^{1/2}x &\leq 0, \hspace{0.5cm} \forall x\in\mathbb{R}^n
    \end{align*}
    所以
    \[A\preccurlyeq \frac{ww^T}{\sum_i w_i}\]
\end{proof}
\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
