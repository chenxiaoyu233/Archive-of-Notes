\documentclass{article}
\usepackage{ctex}
\usepackage{amsmath, amssymb, amsthm}
\usepackage{unicode-math}
\usepackage{tikz}
\usepackage{geometry}

% 修改页面占比大小
\geometry{scale=0.9}

% theorem environment
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{define}{Define}
\newtheorem{fact}{Fact}
\newtheorem{observation}{Observation}

% math setting
\everymath{\displaystyle}
\def\~{\sim}
\def\R{\mathbb{R}}
\def\E_#1#2{\underset{#1}{\mathbb{E}}\left[#2\right]}
\def\<{\langle}
\def\>{\rangle}


\title{Rapid Mixing on Matroid Bases}
\author{Xiaoyu Chen}
\date{}

\begin{document}
\maketitle
最近仔细看了一下Nima Anari在Georia Tech的讲座, 
Nima在讲座上从人的正常思维的顺序给出了证明rapid mixing的思路,
感觉很受启发.

\section{Preliminary}
之前我学过的用于度量两个分布的距离的方法实际上可以被统一到同一个框架下:
\begin{define}[$f$-divergence]
 \ %$f$-divergence could be seen as an approximation of total variation distance.
  
 \begin{tabular}{l l}
   (1) & $f: [0, +\infty) \to \R$ is convex \\
   (2) & $\displaystyle D_f(\nu\parallel\mu) := \E_{S\~\mu}{f(\frac{\nu(S)}{\mu(S)})} - f\left(\E_{S\~\mu}{\frac{\nu(S)}{\mu(S)}}\right)$
 \end{tabular}

  Since $f$ is convex , by Jensen's Inequality, we have
  \[D_f(\nu\parallel\mu) \geq 0, \hspace{0.5cm} \forall \nu, \mu\]
\end{define}

{\flushleft 定义好$f$-divergence之后, 立即就有: }
\begin{theorem}[Implyed by Data Processing Inequality]
  \[D_f(\nu P \parallel \mu P) \leq D_f(\nu \parallel \mu)\]
  holds for any random walk operator $P$.
\end{theorem}
\begin{proof}[Proof]
  个人感觉这个不等式不是不是特别像Data Processing Inequality, 反而直接通过Jesen's Inequality来证明比较简单.
  首先发现不等式两边都有$f(1)$, 我们可以直接去掉, 所以实际上我们要证明的东西是
  \[\E_{S\~\mu P}{f(\frac{\nu P(S)}{\mu P(S)})} \leq \E_{S\~\mu}{f(\frac{\nu(S)}{\mu(S)})}\]
  注意到当$S$固定时, $\frac{\nu P(S)}{\mu P(S)}$实际上可以看成在一个分布$\pi$上求$\frac{\nu}{\mu}$的期望值
  \begin{align*}
    \frac{\nu P(S)}{\mu P(S)}
    &= \frac{\sum_{x} \nu(x) P(x, S)}{\mu P(S)} \\
    &= \sum_x\frac{\mu(x) P(x, S) }{\mu P(S)} \frac{\nu(x)}{\mu(x)}\\
    &= \sum_{x} \pi(x) \frac{\nu(x)}{\mu(x)}
  \end{align*}
  所以上面不等式的LHS可以写成:
  \begin{align*}
    \E_{S\~\mu P}{f(\frac{\nu P(S)}{\mu P(S)})}
    &= \E_{S\~\mu P}{f(\E_{x\~\pi}{\frac{\nu(x)}{\mu(x)}})} \\
    &\leq \E_{S\~\mu P}{\E_{x\~\pi}{f(\frac{\nu(x)}{\mu(x)})}}, \hspace{0.5cm} \mbox{Jensen's Inequality} \\
    &= \sum_{S} \mu P(S) \sum_{x} \frac{\mu(x)P(x,S)}{\mu P(S)} f(\frac{\nu(x)}{\mu(x)}) \\
    &= \E_{x\~\mu}{f(\frac{\nu(x)}{\mu(x)})} \qedhere
  \end{align*}
\end{proof}

\begin{fact} \ \\
  \begin{tabular}{lll}
    (1) & $f(x) = \frac{1}{2}|x-1|$ & $D_f(\nu\parallel\mu)$ is the total variation distance of $\nu$ and $\mu$. \\
    (2) & $f(x) = x^2$ & $D_f(\nu\parallel\mu)$ is the variance of $\frac{\nu}{\mu}$ w.r.t. $\mu$. \\
    (3) & $f(x) = x\log x$ & $D_f(\nu\parallel\mu)$ is the KL-divergence of $\nu$ and $\mu$.
  \end{tabular}
\end{fact}

\begin{fact}
  \[D_f(\nu\parallel\mu) = \E_{S\~\mu}{f(\frac{\nu(S)}{\mu(S)})} - f(1)\]
  有些地方喜欢取满足$f(1) = 0$的$f$, 这样可以简化计算.
\end{fact}

\section{Proof of Rapid Mixing}
记拟阵中大小为$i$的独立集的集合为$\mathcal{M}(i)$.
记$\mathcal{M}(i)$中的独立集的分布为$\mu_i:\mathcal{M}(i)\to\R_{\geq 0}$.
记$r$为拟阵的秩, 则$\mu_r$就是拟阵基的分布.
\begin{fact}[Normalizing]
  $\forall x \in \mathcal{M}(k)$, $\mu_k(x) = \frac{1}{\binom{r}{k}}\sum_{\substack{y\in\mathcal{M}(r)\\y\supseteq x}}\mu_r(y)$
\end{fact}
通过telescope sum, 我们可以将$D_f(\nu_r\parallel \mu_r)$ 进行\textcolor{blue}{纵向}的分解.
\begin{align*}
  D_f(\nu_r \parallel \mu_r)
  &= \E_{S\~\mu_r}{f(\frac{\nu_r(S)}{\mu_r(S)})} - f(1) \\
  &= \sum_{i=1}^r (\E_{S\~\mu_i}{f(\frac{\nu_i(S)}{\mu_i(S)})} - \E_{S\~\mu_{i-1}}{f(\frac{\nu_{i-1}(S)}{\mu_{i-1}(S)})})
\end{align*}
方便起见, 不妨令
\[A_i = \underbrace{\E_{S\~\mu_i}{f(\frac{\nu_i(S)}{\mu_i(S)})}}_{a_i} - \underbrace{\E_{S\~\mu_{i-1}}{f(\frac{\nu_{i-1}(S)}{\mu_{i-1}(S)})}}_{a_{i-1}}\]
则上面的式子可以简写为
\[D_f(\nu_r \parallel \mu_r) = \sum_{i=1}^r A_i\]


\begin{fact}
  \[D_f(\nu_{r-1} \parallel \mu_{r-1}) = D_f(\nu_rP_r^\downarrow\parallel\mu_rP_r^\downarrow)\]
\end{fact}

\begin{observation}
  If $A_i \geq A_{i-1}, i = 2, \cdots, r$, then
  \[D_f(\nu_rP_r^{\downarrow}\parallel \mu_rP_r^{\downarrow}) \leq (1 - \frac{1}{r}) D_f(\nu_r\parallel \mu_r)\]
  where $P_r^\downarrow$ represent the random walk which remove 1 element u.a.r.
\end{observation}
\begin{proof}[Proof]
  显然, $D_f(\nu_r \parallel \mu_r) - D_f(\nu_rP^\downarrow \parallel \mu_rP^\downarrow) = A_r$.
  然后, 因为 $A_i\geq A_{i-1}, i = 2,\cdots, r$, 所以
  \[A_r \geq \frac{1}{r} \sum_{i=1}^r A_i = \frac{1}{r} D_f(\nu_r \parallel \mu_r)\]
  所以
  \begin{align*}
    D_f(\nu_rP^\downarrow \parallel \mu_rP^\downarrow)
    &= D_f(\nu_r \parallel \mu_r) - A_r \\
    &\leq D_f(\nu_r \parallel \mu_r) -  \frac{1}{r} D_f(\nu_r \parallel \mu_r)\\
    &= (1 - \frac{1}{r}) D_f(\nu_r \parallel \mu_r) \qedhere
  \end{align*}
\end{proof}

{\flushleft 不难发现, 只要证明 $A_i \geq A_{i-1}$, 就能够证明rapid mixing, 得到有意义的mixing time.}

\section{Prove $A_i \geq A_{i-1}$ when [$f = x\log x$]}
\begin{align*}
  A_i &\geq A_{i-1} \\
  a_i - a_{i-1} & \geq a_{i-1} - a_{i-2} \\
\end{align*}
所以, 我们只需证明 $a_i + a_{i-2} \geq 2a_{i-1}$ 即可.
故只需证明:
\begin{align*}
  \E_{\mu_i}{f(\frac{\nu_i}{\mu_i})} + \E_{\mu_{i-2}}{f(\frac{\nu_{i-2}}{\mu_{i-2}})} &\geq 2\E_{\mu_{i-1}}{f(\frac{\nu_{i-1}}{\mu_{i-1}})}
\end{align*}
因为
\[\E_{\mu_{i-2}}{f(\frac{\nu_{i-2}}{\mu_{i-2}})} \geq f(1) = 0\]
所以实际上只用证明
\[
  \E_{\mu_i}{f(\frac{\nu_i}{\mu_i})} \geq 2\E_{\mu_{i-1}}{f(\frac{\nu_{i-1}}{\mu_{i-1}})}
\]
证明这个式子通常使用的方法都是将其在\textcolor{blue}{水平}方向上进行分解.
\begin{align*}
  \sum_{J\in\mathcal{M}(i)} \mu_i(J) f(\frac{\nu_i(J)}{\mu_i(J)}) &\geq 2\sum_{J\in\mathcal{M}(i-1)} \mu_{i-1}(J) f(\frac{\nu_{i-1}(J)}{\mu_{i-1}(J)}) \\
  \underbrace{\sum_{J\in\mathcal{M}(i)} \nu_i(J) \log \frac{\nu_i(J)}{\mu_i(J)}}_{(1)} &\geq \underbrace{2\sum_{J\in\mathcal{M}(i-1)} \nu_{i-1}(J) \log \frac{\nu_{i-1}(J)}{\mu_{i-1}(J)}}_{(2)}
\end{align*}
\begin{align*}
  (1) &= \sum_{J\in \mathcal{M}(i)} \frac{1}{\binom{i}{2}} \sum_{\substack{I\in\mathcal{M}(i-2)\\I\subseteq J}}\nu_i(J)\log \frac{\nu_i(J)}{\mu_i(J)}, \hspace{0.5cm} \mbox{$I\subseteq J \land I\in\mathcal{M}(i-2)$ 这样的 $I$ 有 $\binom{i}{2}$个} \\
      &= \sum_{I\in\mathcal{M}(i-2)}\sum_{\substack{J\in\mathcal{M}(i)\\J\supseteq I}} \frac{1}{\binom{i}{2}} \nu_i(J) \log \frac{\nu_i(J)}{\mu_i(J)} \\
      &= 2\sum_{I\in\mathcal{M}(i-2)}\nu_{i-2}(I)\sum_{\substack{\{a,b\}\in\mathcal{M}_I(2)\\J=I\cup\{a,b\}}} \nu_{I, 2}(\{a,b\}) \log \frac{\nu_i(J)}{\mu_i(J)}, \hspace{0.5cm} \mbox{$\nu_{i-2}(I) = \frac{(i-2)!}{i!}\sum_{\substack{J\in\mathcal{M}(i)\\J\supseteq I}}\nu_J(J)$} \\
      &= 2\sum_{I\in\mathcal{M}(i-2)}\nu_{i-2}(I)\sum_{\substack{\{a,b\}\in\mathcal{M}_I(2)\\J=I\cup\{a,b\}}} \nu_{I, 2}(\{a,b\}) \left[\log \frac{\nu_{I,2}(\{a,b\})}{\mu_{I,2}(\{a,b\})} + \log\frac{\nu_{i-2}(I)}{\mu_{i-2}(I)}\right] \\
      &= 2\sum_{I\in\mathcal{M}(i-2)}\nu_{i-2}(I)\sum_{\substack{\{a,b\}\in\mathcal{M}_I(2)\\J=I\cup\{a,b\}}} \nu_{I, 2}(\{a,b\}) \log \frac{\nu_{I,2}(\{a,b\})}{\mu_{I,2}(\{a,b\})} + 2\sum_{I\in\mathcal{M}(i-2)} \nu_{i-2}(I)\log \frac{\nu_{i-2}(I)}{\mu_{i-2}(I)}
\end{align*}
\begin{align*}
  (2) &= 2\sum_{J\in\mathcal{M}(i-1)} \frac{1}{i-1} \sum_{\substack{I\in\mathcal{M}(i-2)\\I\subseteq J}} \nu_{i-1}(J) \log \frac{\nu_{i-1}(J)}{\mu_{i-1}(J)}, \hspace{0.5cm} \mbox{$I\subseteq J \land I\in\mathcal{M}(i-2)$ 这样的 $I$ 有 $i-1$个} \\ 
      &= 2 \sum_{\substack{I\in\mathcal{M}(i-2)}} \sum_{\substack{J\in\mathcal{M}(i-1)\\ J\supseteq I}} \frac{1}{i-1} \nu_{i-1}(J) \log \frac{\nu_{i-1}(J)}{\mu_{i-1}(J)} \\
      &= 2 \sum_{\substack{I\in\mathcal{M}(i-2)}} \nu_{i-2}(I) \sum_{\substack{a\in\mathcal{M}_I(1)\\ J=I\cup\{a\}}}  \nu_{I,1}(\{a\}) \log \frac{\nu_{i-1}(J)}{\mu_{i-1}(J)}, \hspace{0.5cm} \nu_{i-2}(I) = \frac{(i-2)!}{(i-1)!} \sum_{\substack{J\in\mathcal{M}(i-1)\\J\supseteq I}} \nu_{i-1}(J)\\
      &= 2 \sum_{\substack{I\in\mathcal{M}(i-2)}} \nu_{i-2}(I) \sum_{\substack{a\in\mathcal{M}_I(1)\\ J=I\cup\{a\}}}  \nu_{I,1}(\{a\}) \left[\log \frac{\nu_{I,1}(\{a\})}{\mu_{I,1}(\{a\})} + \log \frac{\nu_{i-2}(I)}{\mu_{i-2}(I)} \right] \\
      &= 2 \sum_{\substack{I\in\mathcal{M}(i-2)}} \nu_{i-2}(I) \sum_{\substack{a\in\mathcal{M}_I(1)\\ J=I\cup\{a\}}}  \nu_{I,1}(\{a\}) \log \frac{\nu_{I,1}(\{a\})}{\mu_{I,1}(\{a\})} + 2\sum_{I\in\mathcal{M}(i-2)} \nu_{i-2}(I)\log \frac{\nu_{i-2}(I)}{\mu_{i-2}(I)} 
\end{align*}
比较 $(1), (2)$, 实际上, 最终我们只需要证明 $\forall I\in\mathcal{M}(i-2)$都有: 
\begin{align*}
  \sum_{\{a,b\}\in\mathcal{M}_I(2)} \nu_{I, 2}(\{a,b\}) \log \frac{\nu_{I,2}(\{a,b\})}{\mu_{I,2}(\{a,b\})} \geq \sum_{a\in\mathcal{M}_I(1)}  \nu_{I,1}(\{a\}) \log \frac{\nu_{I,1}(\{a\})}{\mu_{I,1}(\{a\})} 
\end{align*}
Intuitively, 我们将原来要证明的期望不等式分解到了每一个$I\in\mathcal{M}(i-2)$ 上 ($\mathcal{M}_I$的$1,2$层上).
这样分解的原因是:
\begin{align*}
   \mu_{I,2}(\{a,b\}) &\propto \nabla^2\partial_I g_\mu (a, b)\\
   \mu_{I,1}(\{a\}) &\propto \nabla \partial_I g_\mu (a)
\end{align*}
这样分解可以方便我们将 $\partial_I g_\mu$ 是 log-concave 的这个性质拿过来用.

下面继续来证明上面的式子, 将右侧放到2倍可以得到一个更紧的不等式, 这个不等式要好证明一些.
\begin{align*}
  \sum_{\{a,b\}\in\mathcal{M}_I(2)} \nu_{I, 2}(\{a,b\}) \log \frac{\nu_{I,2}(\{a,b\})}{\mu_{I,2}(\{a,b\})} \geq 2\sum_{a\in\mathcal{M}_I(1)}  \nu_{I,1}(\{a\}) \log \frac{\nu_{I,1}(\{a\})}{\mu_{I,1}(\{a\})} 
\end{align*}
观察可以发现:
\begin{align*}
  \nu_{I,1}(\{a\}) = \frac{1}{2}\sum_{\{a,b\}\in\mathcal{M}_I(2)} \nu_{I,2}(\{a,b\})
\end{align*}
所以上面的式子可以继续变形:
\begin{align*}
  \sum_{\{a,b\}\in\mathcal{M}_I(2)} \nu_{I, 2}(\{a,b\}) \log \frac{\nu_{I,2}(\{a,b\})}{\mu_{I,2}(\{a,b\})} &\geq \sum_{a\in\mathcal{M}_I(1)} \sum_{\{a,b\}\in\mathcal{M}_I(2)} \nu_{I,2}(\{a, b\}) \log \frac{\nu_{I,1}(\{a\})}{\mu_{I,1}(\{a\})} \\
  &\geq \sum_{\{a,b\}\in\mathcal{M}_I(2)}    \nu_{I,2}(\{a, b\})\sum_{a\in\{a,b\}}  \log \frac{\nu_{I,1}(\{a\})}{\mu_{I,1}(\{a\})} \\
  &\geq \sum_{\{a,b\}\in\mathcal{M}_I(2)} \nu_{I,2}(\{a, b\})\log\left[\frac{\nu_{I,1}(\{a\})}{\mu_{I,1}(\{a\})}\cdot\frac{\nu_{I,1}(\{b\})}{\mu_{I,1}(\{b\})}\right]
\end{align*}
所以, 我们只需要证明:
\begin{align*}
  \sum_{\{a,b\}\in\mathcal{M}_I(2)}\nu_{I,2}(\{a,b\}) \left(\log \frac{\nu_{I,2}(\{a,b\})}{\mu_{I,2}(\{a,b\})} - \log\left[\frac{\nu_{I,1}(\{a\})}{\mu_{I,1}(\{a\})}\cdot\frac{\nu_{I,1}(\{b\})}{\mu_{I,1}(\{b\})}\right]\right) &\geq 0\\
  \sum_{\{a,b\}\in\mathcal{M}_I(2)} \mu_{I,2}(\{a,b\}) \frac{\nu_{I,2}(\{a,b\})}{\mu_{I,2}(\{a,b\})} \left(\log \frac{\nu_{I,2}(\{a,b\})}{\mu_{I,2}(\{a,b\})} - \log\left[\frac{\nu_{I,1}(\{a\})}{\mu_{I,1}(\{a\})}\cdot\frac{\nu_{I,1}(\{b\})}{\mu_{I,1}(\{b\})}\right]\right) &\geq 0
\end{align*}
因为 $a\log\frac{a}{b} \geq a - b$, 所以上面的式子可以化成
\begin{align*}
  \sum_{\{a,b\}\in\mathcal{M}_I(2)} \mu_{I,2}(\{a,b\}) \left(\frac{\nu_{I,2}(\{a,b\})}{\mu_{I,2}(\{a,b\})} - \frac{\nu_{I,1}(\{a\})}{\mu_{I,1}(\{a\})}\cdot\frac{\nu_{I,1}(\{b\})}{\mu_{I,1}(\{b\})} \right) &\geq 0\\
  \sum_{\{a,b\}\in\mathcal{M}_I(2)} \mu_{I,2}(\{a,b\})\frac{\nu_{I,1}(\{a\})}{\mu_{I,1}(\{a\})}\cdot\frac{\nu_{I,1}(\{b\})}{\mu_{I,1}(\{b\})} &\leq 1
\end{align*}
令$W_I(a, b) = \frac{1}{2}\mu_{I,2}(\{a,b\})$, 我们可以发现 $W_I \propto \nabla^2\partial_I g_\mu$.
不妨令 $W_I = \gamma \nabla^2\partial_I g_\mu$.
\begin{fact}
  if $g_\mu$ is log-concave, then $W_I$ has at most one positive eigenvalue.
\end{fact}
\begin{proof}[Proof]
  首先不难发现 $\nabla^2\partial_I g_\mu$ 最多只有一个正特征值.
  又因为
  \begin{align*}
    \mathrm{Det}(\nabla^2\partial_I g_\mu - \lambda I) &= 0 \\
    \mathrm{Det}(\gamma\nabla^2\partial_I g_\mu - \gamma\lambda I) &= 0 \\
    \mathrm{Det}(W_I - \gamma\lambda I) &= 0 
  \end{align*}
  所以其实 $W_I$的特征方程和$\nabla^2\partial_Ig_\mu$的特征方程之间只差一个常数, 所以$W_I$当然也最多只有一个正特征值.
\end{proof}
所以上面的式子可以进一步写成
\begin{align*}
  x^T W_I x \leq 1, \hspace{0.5cm} \mbox{where, $x = \frac{v_{I,1}}{\mu_{I,1}}$.}
\end{align*}
不妨假设$W_I\in\mathbb{R}^{n\times n}_{\geq 0}$是一个$n\times n$的矩阵[这个矩阵的大小不影响后面的逻辑].
不难发现:
\begin{align*}
  \sum_{j=1}^n W_I(i, j) = \mu_{I,1}(\{i\}) = \sum_{j=1}^n W_I(j, i)
\end{align*}
\begin{lemma}
  如果$M\in\mathbb{R}^{n\times n}$ 是一个对称矩阵, 且$M$最多只有一个正特征值.
  则对于任意的$x, v\in\mathbb{R}^n$, 都有
  \[(x^TMx)(v^TMv)\leq (x^TMv)^2\]
\end{lemma}
\begin{proof}[Proof]
  令 $S = [x | v]$ 则
  \begin{align*}
    A = S^TMS &= \left[
            \begin{array}{c}
              x^T \\
              \hline
              v^T
            \end{array}
    \right] M \left[x | v \right] \\
    &= \left[
    \begin{array}{cc}
      x^TMx & x^TMv \\
      v^TMx & v^TMv
    \end{array}
    \right]
  \end{align*}
  因为$M$最多有一个正特征值, 所以$A$最多有一个正特征值.
  又因为 $A$ 是一个正项矩阵, 所以 $A$ 一定有一个正特征值.
  所以$A$的另外一个特征值$\leq 0$.
  所以
  \begin{align*}
    \lambda_1\lambda_2 = \mathrm{Det}(A) & \leq 0 \\
    (x^TMx)(v^TMv) &\leq (x^TMv)^2 \qedhere
  \end{align*}
\end{proof}
所以
\begin{align*}
  (x^TW_Ix)(\mathbb{1}^TW_I\mathbb{1}) &\leq (x^TM\mathbb{1})^2 \\
  x^TW_Ix &\leq 1
\end{align*}
\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
